{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awFM1ykiKKY8"
      },
      "source": [
        "## Thin Plate Spline [1], LoFTR [2] and CRF Post-Processing [3] for image matching and warping\n",
        "\n",
        "### References\n",
        "[1] Donato, G., & Belongie, S. J. (2003). Approximation methods for thin plate spline mappings and principal warps. Department of Computer Science and Engineering, University of California, San Diego. - https://github.com/cheind/py-thin-plate-spline\n",
        "\n",
        "[2] Sun, Jiaming and Shen, Zehong and Wang, Yuang and Bao, Hujun and Zhou, Xiaowei (2021). LoFTR: Detector-Free Local Feature Matching with Transformers - https://github.com/zju3dv/LoFTR\n",
        "\n",
        "[3] http://web.archive.org/web/20161023180357/http://www.philkr.net/home/densecrf, https://github.com/lucasb-eyer/pydensecrf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and installation"
      ],
      "metadata": {
        "id": "glCbfIyQugVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should have TPS_LoFTR folder created in your drive. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ubxxWjwSulYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ATOlcDdyK-sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/SemesterProj/TPS_LoFTR"
      ],
      "metadata": {
        "id": "WCBVBZRvLSrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries and download weights for both indoor and outdoor pretrained models on MegaDepth dataset.\n",
        "\n",
        "**IMPORTANT**: The original LoFTR code has been altered, therefore you do not need to clone the original LoFTR repo anymore. However, you could still do this by running the commands below:\n",
        "```\n",
        "!git clone https://github.com/zju3dv/LoFTR --depth 1\n",
        "!mv LoFTR/* . && rm -rf LoFTR\n",
        "```"
      ],
      "metadata": {
        "id": "ofCo-a44vXeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "!pip install torch einops yacs kornia"
      ],
      "metadata": {
        "id": "9eKknOCveS7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained weights for LoFTR\n",
        "!mkdir weights \n",
        "%cd weights/\n",
        "!gdown --id 1w1Qhea3WLRMS81Vod_k5rxS_GNRgIi-O  # indoor-ds\n",
        "!gdown --id 1M-VD35-qdB5Iw-AtbDBCKC7hPolFW9UY  # outdoor-ds\n",
        "%cd .."
      ],
      "metadata": {
        "id": "E5u8-aYwdmB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the needed libraries."
      ],
      "metadata": {
        "id": "Oqariz_1vroK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUpYFDPnKKY_"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import cv2\n",
        "from src.utils.plotting import make_matching_figure\n",
        "from src.loftr import LoFTR, default_cfg\n",
        "import thinplate as tps\n",
        "import os\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral, unary_from_labels\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "-8Pin2y_zn1V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9CIu8LjKKZD"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amkit0fHKKZD"
      },
      "outputs": [],
      "source": [
        "def show_warped(img, warped):\n",
        "    \"\"\"\n",
        "    Plot images.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    axs[0].imshow(img[...,::-1], origin='upper')\n",
        "    axs[1].imshow(warped[...,::-1], origin='upper')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def warp_image_cv(img, c_src, c_dst, dshape=None, interp='cubic'):\n",
        "    \"\"\"\n",
        "    Function for warping satellite labels based on matched point sets in\n",
        "    satellite and drone images. Uses Thin-Plate-Spline transformation.\n",
        "    \"\"\"\n",
        "\n",
        "    dshape = dshape or img.shape\n",
        "\n",
        "    # Find parameters\n",
        "    theta = tps.tps_theta_from_points(c_src, c_dst, reduced=True)\n",
        "\n",
        "    # Create grid and remap according to interpolation method\n",
        "    grid = tps.tps_grid(theta, c_dst, dshape)\n",
        "    mapx, mapy = tps.tps_grid_to_remap(grid, img.shape)\n",
        "\n",
        "    if interp == 'cubic':\n",
        "        return cv2.remap(img, mapx, mapy, cv2.INTER_CUBIC)\n",
        "    elif interp == 'nn':\n",
        "        return cv2.remap(img, mapx, mapy, cv2.INTER_NEAREST)\n",
        "\n",
        "\n",
        "def threshold_img(img_hsv):\n",
        "    \"\"\" \n",
        "    Function for color thresholding and categorical conversion, based on HSV colorspace.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define range of blue color in HSV\n",
        "    lower_blue = np.array([115, 50, 50])\n",
        "    upper_blue = np.array([125, 255, 255])\n",
        "    mask_blue = cv2.inRange(img_hsv, lower_blue, upper_blue)\n",
        "\n",
        "    # Define range of green color in HSV\n",
        "    lower_green = np.array([55, 50, 50])\n",
        "    upper_green = np.array([65, 255, 255])\n",
        "    mask_green = cv2.inRange(img_hsv, lower_green, upper_green)\n",
        "\n",
        "    # Define range for red lower mask (0-10)\n",
        "    lower_red = np.array([0, 50, 50])\n",
        "    upper_red = np.array([5, 255, 255])\n",
        "    mask0_red = cv2.inRange(img_hsv, lower_red, upper_red)\n",
        "\n",
        "    # Define range for red upper mask (170-180)\n",
        "    lower_red = np.array([175, 50, 50])\n",
        "    upper_red = np.array([180, 255, 255])\n",
        "    mask1_red = cv2.inRange(img_hsv, lower_red, upper_red)\n",
        "\n",
        "    # Join red masks\n",
        "    mask_red = mask0_red + mask1_red\n",
        "\n",
        "    # Create categorical output based on mask\n",
        "    output_hsv = np.zeros((img_hsv.shape[0], img_hsv.shape[1]))\n",
        "    output_hsv[np.where(mask_red == 255)] = 1\n",
        "    output_hsv[np.where(mask_blue == 255)] = 2\n",
        "    output_hsv[np.where(mask_green == 255)] = 3\n",
        "\n",
        "    return output_hsv.astype(np.uint8)\n",
        "\n",
        "\n",
        "def draw_matches(mconf, mkpts0, mkpts1, img0_raw, img1_raw, download_pdf=False):\n",
        "    \"\"\" Draw matches found by LoFTR on satellite and drone images.\"\"\"\n",
        "    \n",
        "    # Define colors and texts for the plot\n",
        "    color = cm.jet(mconf, alpha=0.7)\n",
        "    text = [\n",
        "        'LoFTR',\n",
        "        'Matches: {}'.format(len(mkpts0)),\n",
        "    ]\n",
        "    fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text)\n",
        "\n",
        "    # A high-res PDF will also be downloaded automatically.\n",
        "    if download_pdf:\n",
        "        make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text, path=\"LoFTR-colab-demo.pdf\")\n",
        "        files.download(\"LoFTR-colab-demo.pdf\")\n",
        "    else:\n",
        "        make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, mkpts0, mkpts1, text)#, path=\"LoFTR-colab-demo.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration and setup"
      ],
      "metadata": {
        "id": "IB50lq5J2Qeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All data directory\n",
        "data_dir = '../data'\n",
        "\n",
        "# Current dataset directory\n",
        "dataset_dir = '120m'\n",
        "\n",
        "# Satellite images directory name\n",
        "img_dir = \"img_dir\"\n",
        "\n",
        "# Labels directory name\n",
        "ann_dir = \"ann_dir\"\n",
        "\n",
        "# Drone images directory name\n",
        "drone_dir = 'drone_dir'\n",
        "\n",
        "# Maximum number of matches to keep from LoFTR (after fine grained module)\n",
        "nr_conf = 1000\n",
        "\n",
        "# Visualizations of matched points and results\n",
        "show = False\n",
        "\n",
        "# Inference image size - (640, 480) BY DEFAULT IN ORIGINAL LoFTR\n",
        "inference_im_size = (480, 360)\n",
        "\n",
        "# Output image size\n",
        "output_im_size = (5280, 3956)\n",
        "fullsize = (output_im_size[0] == 5280 and output_im_size[1] == 3956)\n",
        "\n",
        "# Palette and classes\n",
        "classes = ['background', 'building', 'road', 'water']\n",
        "palette = [[0, 0, 0], [255, 0, 0], [0, 0, 255], [0, 255, 0]]\n",
        "\n",
        "# Define the list of drone image names that you want to warp\n",
        "files = sorted(os.listdir(os.path.join(dataset_root, \"human_drone_ann_dir\")))\n",
        "\n",
        "# Interpolation method:\n",
        "    # 'nn' - for nearest neighbour\n",
        "    # 'cubic' - for cubic interpolation and HSV color thresholding\n",
        "interp_type = 'cubic'\n",
        "\n",
        "# Directory where to save the warped labels ( and create if not exists)\n",
        "output_dir = os.path.join(dataset_root, \n",
        "                          'loftr_warped{}_infer_{}_{}_{}_{}p'.format(\n",
        "                              '_fullsize' if fullsize else '', interp_type,\n",
        "                               inference_im_size[0], inference_im_size[1], nr_conf)\n",
        "                          )\n",
        "\n",
        "# Create directories and paths\n",
        "dir_name = os.path.expanduser(output_dir)\n",
        "os.makedirs(dir_name, mode=0o777, exist_ok=True)\n",
        "data_root = os.path.join(data_dir, dataset_dir)\n",
        "img_root = os.path.join(data_root, img_dir)\n",
        "ann_root = os.path.join(data_root, ann_dir)\n",
        "drone_root = os.path.join(data_root, drone_dir)"
      ],
      "metadata": {
        "id": "xH2sjRPb2zZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default config uses dual-softmax.\n",
        "The outdoor and indoor models share the same config.\n",
        "You can change the default values like ```image_type``` and ```thr``` in the config dictionary."
      ],
      "metadata": {
        "id": "XpxTBNnWy2bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set type of model - indoor/outdoor\n",
        "image_type = 'outdoor'\n",
        "\n",
        "# Set threshold for coarse matching module - default 0.2\n",
        "default_cfg['match_coarse']['thr'] = 0.85\n",
        "\n",
        "# Window size search space on confidence matrix\n",
        "    # Conf_matrix will be of size: im_size / 8, therefore a window size of im_size / 4 will cover the whole\n",
        "    # confidence matrix because the window is defined around each point in the confidence matrix.\n",
        "    # Now it is set to whole confidence matrix, so no filtering (1 is maximum)\n",
        "filter_window_size = 1\n",
        "default_cfg['match_coarse']['filter_area'] = str(int(inference_im_size[0] / 4 / filter_window_size)) \n",
        "\n",
        "# Define model\n",
        "matcher = LoFTR(config=default_cfg)\n",
        "if image_type == 'indoor':\n",
        "  matcher.load_state_dict(torch.load(\"weights/indoor_ds.ckpt\")['state_dict'])\n",
        "elif image_type == 'outdoor':\n",
        "  matcher.load_state_dict(torch.load(\"weights/outdoor_ds.ckpt\")['state_dict'])\n",
        "else:\n",
        "  raise ValueError(\"Wrong image_type is given.\")\n",
        "matcher = matcher.eval().cuda()"
      ],
      "metadata": {
        "id": "IqPld6cihKJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference and warping"
      ],
      "metadata": {
        "id": "xd79wkKC22pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, filename in enumerate(files):\n",
        "    img_name = filename.split('.')[0]\n",
        "\n",
        "    # Define drone and satellite image paths and create pair\n",
        "    img0_pth = os.path.join(drone_root, '{}.jpg'.format(img_name))\n",
        "    img1_pth = os.path.join(img_root, '{}.jpg'.format(img_name))\n",
        "    image_pair = [img0_pth, img1_pth]\n",
        "\n",
        "    # Read and resize images\n",
        "    img0_raw = cv2.imread(image_pair[0], cv2.IMREAD_GRAYSCALE)\n",
        "    img1_raw = cv2.imread(image_pair[1], cv2.IMREAD_GRAYSCALE)\n",
        "    img0_raw = cv2.resize(img0_raw, inference_im_size)\n",
        "    img1_raw = cv2.resize(img1_raw, inference_im_size)\n",
        "\n",
        "    # Normalize images and create batch format\n",
        "    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\n",
        "    img1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255.\n",
        "    batch = {'image0': img0, 'image1': img1}\n",
        "\n",
        "    # Inference with LoFTR and get prediction\n",
        "    with torch.no_grad():\n",
        "        matcher(batch)\n",
        "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
        "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
        "        mconf = batch['mconf'].cpu().numpy()\n",
        "\n",
        "    if show:\n",
        "        # Plot matches\n",
        "        draw_matches(mconf, mkpts0, mkpts1, img0_raw, img1_raw)\n",
        "    \n",
        "    # Get satellite label and resize\n",
        "    if interp_type == 'nn':\n",
        "        img = Image.open(os.path.join(ann_root, '{}.png'.format(img_name)))\n",
        "        img = np.array(img.resize(inference_im_size, Image.NEAREST))\n",
        "    else:\n",
        "        img = np.array(Image.open(os.path.join(ann_root, '{}.png'.format(img_name))).convert('RGB'))[:, :, ::-1]\n",
        "        img = cv2.resize(img, inference_im_size)\n",
        "\n",
        "    # Get indexes of the first nr_conf most confident matches\n",
        "    indexes = np.argsort(mconf)[::-1][:nr_conf]\n",
        "\n",
        "    # Define source and destination points for TPS alorithm and normalize them\n",
        "    c_src = mkpts1[indexes, :]\n",
        "    c_dst = mkpts0[indexes, :]\n",
        "\n",
        "    c_src[:, 0] /= img.shape[1]\n",
        "    c_src[:, 1] /= img.shape[0]\n",
        "    c_dst[:, 0] /= img.shape[1]\n",
        "    c_dst[:, 1] /= img.shape[0]\n",
        "\n",
        "    print(\"There are {} keypoints for {}!\".format(len(c_src), img_name))\n",
        "    \n",
        "    # Do the warping only if there are at lest 4 matched points.\n",
        "    # Otherwise, the warping step won't work out.\n",
        "    if len(c_src) >= 4:\n",
        "        warped = warp_image_cv(img, c_src, c_dst, dshape=None, interp=interp_type)\n",
        "        if inference_im_size != output_im_size:\n",
        "            if interp_type == 'nn':\n",
        "                warped = np.array(Image.fromarray(warped).resize(output_im_size, Image.NEAREST))\n",
        "            else:\n",
        "                warped = cv2.resize(warped, output_im_size, interpolation=cv2.INTER_CUBIC)\n",
        "    else:\n",
        "        if inference_im_size != output_im_size:\n",
        "            if interp_type == 'nn':\n",
        "                warped = np.array(Image.fromarray(warped).resize(output_im_size, Image.NEAREST))\n",
        "            else:\n",
        "                warped = cv2.resize(img, output_im_size, interpolation=cv2.INTER_CUBIC)\n",
        "        print(\"No warping\")\n",
        "\n",
        "    if show:\n",
        "        # Plot overlayed images\n",
        "        drone_img = cv2.imread(image_pair[0])\n",
        "        drone_img = cv2.resize(drone_img, output_im_size)\n",
        "        if interp_type == 'nn':\n",
        "            label = Image.open(os.path.join(ann_root,'{}.png'.format(img_name)))\n",
        "            label = np.array(label.resize(output_im_size, Image.NEAREST).convert('RGB'))[:, :, ::-1]\n",
        "            warped_show = Image.fromarray(warped).convert('P')\n",
        "            warped_show.putpalette(np.array(palette, dtype=np.uint8))\n",
        "            warped_show = np.array(warped_show.convert('RGB'))[:, :, ::-1]\n",
        "        else:\n",
        "            label = np.array(Image.open(os.path.join(ann_root,'{}.png'.format(img_name))).convert('RGB'))[:, :, ::-1]\n",
        "            label = cv2.resize(label, output_im_size, interpolation=cv2.INTER_CUBIC)\n",
        "            warped_show = warped.copy()\n",
        "        drone_warped = cv2.addWeighted(drone_img, 0.6, warped_show, 0.7, 0)\n",
        "        drone_satlabel = cv2.addWeighted(drone_img, 0.6, label, 0.7, 0)\n",
        "        show_warped(drone_warped, drone_satlabel)\n",
        "\n",
        "    if interp_type == 'nn':\n",
        "        seg_img = Image.fromarray(warped).convert('P')\n",
        "        seg_img.putpalette(np.array(palette, dtype=np.uint8))\n",
        "    else:\n",
        "        # Threshold warped image on HSV colorspace and convert to 'P' format\n",
        "        warped = cv2.cvtColor(warped, cv2.COLOR_BGR2HSV)\n",
        "        thresh_drone = threshold_img(warped)\n",
        "        seg_img = Image.fromarray(thresh_drone).convert('P')\n",
        "        seg_img.putpalette(np.array(palette, dtype=np.uint8))\n",
        "    \n",
        "    # Save result\n",
        "    seg_img.save(os.path.join(output_dir, filename))"
      ],
      "metadata": {
        "id": "yOn3HODRhK2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF- Post Processing\n",
        "\n",
        "More on CRF at:  https://github.com/lucasb-eyer/pydensecrf"
      ],
      "metadata": {
        "id": "rhj1tLk2MFWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "metadata": {
        "id": "aJaqH8UpufbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CRF parameters\n",
        "compat_gauss = 3 # Higher, for smoother mask\n",
        "sxy_gauss = 3\n",
        "comapt_bil = 10\n",
        "sxy_bil = 80 # Pixel proximity, care about change of intensity \n",
        "srgb_bil = 13 # Discard difference in pixel intensity if higher, the mask gets cut off if too high\n",
        "\n",
        "# Number of iterations of the algorithm\n",
        "iters = 10\n",
        "\n",
        "# The certainty of the ground-truth (or reference label), must be between (0, 1) - manually chosen\n",
        "gtprob = 0.7\n",
        "\n",
        "# Path to labels\n",
        "ann_path = '../data/120m/loftr_warped_fullsize'\n",
        "\n",
        "# Path to drone images\n",
        "drone_path = '../data/120m/drone_dir'"
      ],
      "metadata": {
        "id": "aWO-G7IPl29z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-process"
      ],
      "metadata": {
        "id": "0NcfqKTiuiDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = sorted(os.listdir(ann_path))\n",
        "\n",
        "for i, filename in enumerate(files):\n",
        "    # Read drone image and reference annotation\n",
        "    img = np.array(Image.open(os.path.join(drone_path, filename.split('.')[0] + '.jpg')))\n",
        "    ann = np.array(Image.open(os.path.join(ann_path, filename)))\n",
        "\n",
        "    # Compute new image\n",
        "    U = unary_from_labels(ann, 4, gtprob, zero_unsure=False)\n",
        "    d = dcrf.DenseCRF2D(ann.shape[1], ann.shape[0], 4)\n",
        "    d.setUnaryEnergy(U)\n",
        "    d.addPairwiseGaussian(sxy=(sxy_gauss,sxy_gauss), compat=compat_gauss, kernel=dcrf.DIAG_KERNEL, normalization = dcrf.NORMALIZE_SYMMETRIC)\n",
        "    d.addPairwiseBilateral(sxy=(sxy_bil,sxy_bil), srgb=(srgb_bil,srgb_bil,srgb_bil), rgbim=img, compat=comapt_bil, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "    Q = d.inference(iters)\n",
        "    map = np.argmax(Q, axis=0).reshape((ann.shape[0], ann.shape[1]))\n",
        "    proba = np.array(map)\n",
        "\n",
        "    # Convert to 'P' format and save\n",
        "    seg_img = Image.fromarray(proba.astype(np.uint8)).convert('P')\n",
        "    seg_img.putpalette(np.array(palette, dtype=np.uint8))\n",
        "    seg_img.save('test{}_after{}_prob{}_compat_gauss{}_sxy_gauss{}_sxy_bil{}_srgb_bil{}.png'.format(i, str(iters), str(gtprob).split('.')[1], str(compat_gauss), str(sxy_gauss), str(sxy_bil), str(srgb_bil)))\n",
        "    print(\"{} done!\".format(filename))\n",
        "    "
      ],
      "metadata": {
        "id": "Ixn3C4JdfN7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}