{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVmnaxFJvsb8"
      },
      "source": [
        "# MMSegmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook used for Semantic Segmentation task using drone images, satellite images and various warped satellite images.\n",
        "\n",
        "\n",
        "Reference: https://github.com/open-mmlab/mmsegmentation"
      ],
      "metadata": {
        "id": "Pc7T7eqTsiE5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS8YHrEhbpas"
      },
      "source": [
        "## Installation and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaiu4y76mmpR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to have a semester project directory in your drive storage and ```cd``` there."
      ],
      "metadata": {
        "id": "CDZXgj3Bsp6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15ltQbDGnZUj"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/SemesterProj\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "llo5-LEOtEoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that you are using a GPU for execution. Also check available GPU memory."
      ],
      "metadata": {
        "id": "XfJ89O6ds6WX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWyLrLYaNEaL"
      },
      "outputs": [],
      "source": [
        "# Check nvcc version\n",
        "!nvcc -V\n",
        "# Check GCC version\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use PyTorch 1.10 and CUDA 11.1. You may install other versions by change the version number in pip install command, but this is not advised."
      ],
      "metadata": {
        "id": "ZOVUWOla_03m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki3WUBjKbutg"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch\n",
        "!pip install torch==1.12.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "# Install MMCV\n",
        "!pip install openmim\n",
        "!mim install mmcv-full==1.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR-hHRvbNJJZ"
      },
      "outputs": [],
      "source": [
        "# Remove existing mmsegmentation and install new mmsegmentation library\n",
        "!rm -rf mmsegmentation\n",
        "!git clone https://github.com/open-mmlab/mmsegmentation.git \n",
        "%cd mmsegmentation\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check mmseg and torch versions installed."
      ],
      "metadata": {
        "id": "gMeMevkstZ3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAE_h7XhPT7d"
      },
      "outputs": [],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMSegmentation installation\n",
        "import mmseg\n",
        "print(mmseg.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "I1EBefNa5KKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare own dataset format"
      ],
      "metadata": {
        "id": "xgp7_qyXSTmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT NOTE:** You are now in the ```mmsegmentation``` folder which will be installed each time you run the ```Installation and Setup``` section. When your Colab session terminates or breaks this folder will be deleted as you would have to install all the libraries again, as they are depending on the current GPU provided. Therefore, **pay attention because you have to keep your data and saved models or files out of this directory!** For this reason our file structure looks like this:\n",
        "```\n",
        "/content/drive/MyDrive/SemesterProj\n",
        "├── data\n",
        "│   ├── 120m                     <- dataset folder for this notebook\n",
        "│   │   ├── ann_dir              <- satellite labels\n",
        "|   |   |   ├── <img_name_1>.png     \n",
        "│   │   │   └── ...\n",
        "│   │   ├── img_dir              <- satellite images\n",
        "|   |   |   ├── <img_name_1>.jpg    \n",
        "│   │   │   └── ...\n",
        "│   │   ├── human_drone_ann_dir  <- drone labels\n",
        "│   │   ├── drone_dir            <- drone images\n",
        "│   │   ├── splits               <- this folder will be created by code\n",
        "│   │   │   ├── train.txt        <- train images\n",
        "│   │   │   └── val.txt          <- val images\n",
        "│   │   ├── results1             <- other folders with results from predictions or warping\n",
        "|   |   |   ├── <img_name_1>.png   \n",
        "│   │   │   └── ...\n",
        "│   │   └── ...                  <- other folders\n",
        "│   └── ...                      <- other dataset folder\n",
        "├── mmsegmentation               <- previously installed folder for mmsegmentation repo       <- YOU ARE HERE!\n",
        "│   ├── data                     <- symlink to external data folder !!!!!\n",
        "│   ├── mmseg                    <- subfolders of mmsegmentation\n",
        "│   ├── configs                  <- configs folder from mmsegmentation\n",
        "│   ├── checkpoints              <- checkpoints folder from mmsegmentation\n",
        "│   └── ...                      <- subfolders of mmsegmentation\n",
        "└── ...\n",
        "```\n",
        "**NOTE:** You should still have a folder called ```data``` under ```mmsegmentation``` that will point to the external ```data``` folder. We will create a symlink later!"
      ],
      "metadata": {
        "id": "7EJEkcpuuUyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declare variables and paths for dataset configuration and visualization"
      ],
      "metadata": {
        "id": "r0F6YBN6SZkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mmcv\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "import tqdm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os.path as osp\n",
        "from mmseg.datasets.builder import DATASETS\n",
        "from mmseg.datasets.custom import CustomDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
        "from mmseg.core.evaluation import get_palette\n",
        "from mmcv import Config\n",
        "from mmseg.apis import set_random_seed\n",
        "from mmseg.utils import get_device"
      ],
      "metadata": {
        "id": "Sr601-sMDnAO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQI_Y3SL4rxN"
      },
      "source": [
        "Create symlink to external data folder (see explanation in *Prepare own dataset format* section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SOnwb2yv8BjF"
      },
      "outputs": [],
      "source": [
        "os.symlink('/content/drive/MyDrive/SemesterProj/data', 'data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declare classes and palette**"
      ],
      "metadata": {
        "id": "1KVwmt4uCmHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class names used for training\n",
        "classes = ('background', 'building', 'road', 'water')\n",
        "\n",
        "# RGB color for each class\n",
        "palette = [[0, 0, 0], [255, 0, 0], [0, 0, 255], [0, 255, 0]]"
      ],
      "metadata": {
        "id": "rn-vO7OMCjfr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declare variables**"
      ],
      "metadata": {
        "id": "gEd_sanTDvGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87z282VByq0g"
      },
      "outputs": [],
      "source": [
        "# All data directory\n",
        "data_dir = \"data\"\n",
        "\n",
        "# Current dataset directory\n",
        "dataset_dir = \"120m\"\n",
        "\n",
        "# Training samples directory name\n",
        "img_dir = \"img_dir\"\n",
        "\n",
        "# Labels directory name\n",
        "ann_dir = \"ann_dir\"\n",
        "\n",
        "# Splits folder name\n",
        "split_dir = 'splits'\n",
        "\n",
        "# Model used in training or inference ('deeplabv3plus' or 'unet')\n",
        "model_type = 'deeplabv3plus'\n",
        "\n",
        "# Experiment id - for tracking experiments under the same configuration\n",
        "exp_id = 0\n",
        "\n",
        "# Custom image scale used in some experiments\n",
        "custom_img_scale = (1056, 792)\n",
        "\n",
        "data_root = os.path.join(data_dir, dataset_dir)\n",
        "img_root = os.path.join(data_root, img_dir)\n",
        "ann_root = os.path.join(data_root, ann_dir)\n",
        "\n",
        "print(data_root)\n",
        "print(img_root)\n",
        "print(ann_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEoGP-OP4_2v"
      },
      "source": [
        "Look at one training image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78LIci7F9WWI"
      },
      "outputs": [],
      "source": [
        "# Change image name according to your files structure\n",
        "viz_image_name = 'DJI_0339_120'\n",
        "\n",
        "img = mmcv.imread(os.path.join(img_root, viz_image_name + '.jpg'))\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(mmcv.bgr2rgb(img))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWeR3OdFX0xt"
      },
      "source": [
        "Look at one label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MCSS9ABfSks"
      },
      "outputs": [],
      "source": [
        "img = Image.open(os.path.join(ann_root, viz_image_name + '.png'))\n",
        "plt.figure(figsize=(8, 6))\n",
        "im = plt.imshow(np.array(img.convert('RGB')))\n",
        "\n",
        "# create a patch (proxy artist) for every color \n",
        "patches = [mpatches.Patch(color=np.array(palette[i])/255., \n",
        "                          label=classes[i]) for i in range(len(classes))]\n",
        "# put those patched as legend-handles into the legend\n",
        "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \n",
        "           fontsize='large')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HchvmGYB_rrO"
      },
      "source": [
        "Declare our own dataset subclass, here `SatelliteDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LbsWOw62_o-X"
      },
      "outputs": [],
      "source": [
        "@DATASETS.register_module()\n",
        "class SatelliteDataset(CustomDataset):\n",
        "  CLASSES = classes\n",
        "  PALETTE = palette\n",
        "  def __init__(self, split, ignore_index, **kwargs):\n",
        "    super().__init__(img_suffix='.jpg', seg_map_suffix='.png', \n",
        "                     split=split, ignore_index=ignore_index, **kwargs)\n",
        "    assert osp.exists(self.img_dir) and self.split is not None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set and create working directory.It will have the format: exp\\<exp_id>_\\<model_type>_\\<img_dir>_\\<label_dir>_\\<split>\n",
        "\n",
        "**IMPORTANT:** The working directory should be outside mmsegmentation folder."
      ],
      "metadata": {
        "id": "BomefIiGDpzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "work_dir = '../exp{}_{}_{}_{}_{}'.format(exp_id, model_type, img_dir, ann_dir, split_dir)\n",
        "mmcv.mkdir_or_exist(work_dir)\n",
        "print(work_dir)"
      ],
      "metadata": {
        "id": "RX_6unZzDc0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints downloading"
      ],
      "metadata": {
        "id": "v9VktQApB7Mq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjkn6KitrS8j"
      },
      "source": [
        "\n",
        "Check https://github.com/open-mmlab/mmsegmentation/tree/master/configs for a specific pretrained net and a specific checkpoint file (.pth) and replace that filename in the ```wget``` command. We chose a pretrained DeepLabV3+ with a ResNet50 backbone, on CityScapes Dataset and a pretrained UNet+FCN on CityScapes Dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "iCtoNiikCHgq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepLabV3+ checkpoint"
      ],
      "metadata": {
        "id": "T0IDWsFXCy3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6bbsFzzrPnp"
      },
      "outputs": [],
      "source": [
        "!wget https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r50-d8_512x1024_80k_cityscapes/deeplabv3plus_r50-d8_512x1024_80k_cityscapes_20200606_114049-f9fb496d.pth -P checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNet checkpoint"
      ],
      "metadata": {
        "id": "Ypt4oNkNC3_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.openmmlab.com/mmsegmentation/v0.5/unet/fcn_unet_s5-d16_4x4_512x1024_160k_cityscapes/fcn_unet_s5-d16_4x4_512x1024_160k_cityscapes_20211210_145204-6860854e.pth -P checkpoints"
      ],
      "metadata": {
        "id": "tJn7eLe9DAeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUVtmn3Iq3WA"
      },
      "source": [
        "In the next step, we need to modify the config for the training. To accelerate the process, we finetune the model from trained weights.\n",
        "\n",
        "The path to your desired base configuration file should match your checkpoint file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWtija9Xln1O"
      },
      "source": [
        "## Create config file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose base configuration file according to model used and modify according to your dataset. (you may change this function)"
      ],
      "metadata": {
        "id": "gVQ5dPplF18E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eyKnYC1Z7iCV"
      },
      "outputs": [],
      "source": [
        "def create_config(model_type, classes, data_root, img_dir, ann_dir, split_dir, work_dir, custom_img_scale, resume_file=''):\n",
        "\n",
        "    # Give the path to your desired base configuration file\n",
        "    if model_type == 'deeplabv3plus':\n",
        "        cfg = Config.fromfile('configs/deeplabv3plus/deeplabv3plus_r50-d8_512x1024_80k_cityscapes.py')\n",
        "    elif model_type == 'unet':\n",
        "        cfg = Config.fromfile('configs/unet/fcn_unet_s5-d16_4x4_512x1024_160k_cityscapes.py')\n",
        "\n",
        "    # Since we use only one GPU, BN is used instead of SyncBN\n",
        "    cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
        "    cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
        "    cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
        "    cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
        "\n",
        "    # Modify num classes of the model in decode/auxiliary head\n",
        "    cfg.model.decode_head.num_classes = len(classes)\n",
        "    cfg.model.auxiliary_head.num_classes = len(classes)\n",
        "\n",
        "    # For UNet model we need to proceed in a sliding window fashion, when validating\n",
        "    if model_type == 'unet':\n",
        "        cfg.model.test_cfg.mode='slide'\n",
        "        cfg.model.test_cfg.crop_size=(512, 1024)\n",
        "        # Stride is defined according to image size and crop size\n",
        "        cfg.model.test_cfg.stride=(492, 608)\n",
        "\n",
        "    # Modify dataset type and path\n",
        "    cfg.dataset_type = 'SatelliteDataset'\n",
        "    cfg.data_root = data_root\n",
        "\n",
        "    # *Important*: The default learning rate in config files is for 4 GPUs and 2 img/gpu (batch size = 4x2 = 8).\n",
        "    cfg.data.samples_per_gpu = 2\n",
        "    cfg.data.workers_per_gpu= 2\n",
        "\n",
        "    # Cityscapes statistics\n",
        "    cfg.img_norm_cfg = dict(\n",
        "        mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
        "\n",
        "    cfg.crop_size = (1024, 512)\n",
        "    cfg.train_pipeline = [\n",
        "        dict(type='LoadImageFromFile'),\n",
        "        dict(type='LoadAnnotations'),\n",
        "        dict(type='Resize', img_scale=custom_img_scale, ratio_range=(0.5, 2.0)),\n",
        "        dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
        "        dict(type='RandomFlip', flip_ratio=0.5),\n",
        "        dict(type='PhotoMetricDistortion'),\n",
        "        dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "        dict(type='Pad', size=cfg.crop_size, pad_val=0, seg_pad_val=255),\n",
        "        dict(type='DefaultFormatBundle'),\n",
        "        dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n",
        "    ]\n",
        "\n",
        "    cfg.test_pipeline = [\n",
        "        dict(type='LoadImageFromFile'),\n",
        "        dict(\n",
        "            type='MultiScaleFlipAug',\n",
        "            img_scale=custom_img_scale,\n",
        "            # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
        "            flip=False,\n",
        "            transforms=[\n",
        "                dict(type='Resize', keep_ratio=True),\n",
        "                dict(type='RandomFlip'),\n",
        "                dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "                dict(type='ImageToTensor', keys=['img']),\n",
        "                dict(type='Collect', keys=['img']),\n",
        "            ])\n",
        "    ]\n",
        "\n",
        "    cfg.data.train.type = cfg.dataset_type\n",
        "    cfg.data.train.data_root = cfg.data_root\n",
        "    cfg.data.train.img_dir = img_dir\n",
        "    cfg.data.train.ann_dir = ann_dir\n",
        "    cfg.data.train.pipeline = cfg.train_pipeline\n",
        "    cfg.data.train.split = split_dir + '/train.txt'\n",
        "    cfg.data.train.ignore_index = 3\n",
        "\n",
        "    cfg.data.val.type = cfg.dataset_type\n",
        "    cfg.data.val.data_root = cfg.data_root\n",
        "    cfg.data.val.img_dir = img_dir\n",
        "    cfg.data.val.ann_dir = ann_dir\n",
        "    cfg.data.val.pipeline = cfg.test_pipeline\n",
        "    cfg.data.val.split = split_dir + '/val.txt'\n",
        "    cfg.data.val.ignore_index = 3\n",
        "\n",
        "    cfg.data.test.type = cfg.dataset_type\n",
        "    cfg.data.test.data_root = cfg.data_root\n",
        "    cfg.data.test.img_dir = img_dir\n",
        "    cfg.data.test.ann_dir = ann_dir\n",
        "    cfg.data.test.pipeline = cfg.test_pipeline\n",
        "    cfg.data.test.split = split_dir + '/val.txt'\n",
        "    cfg.data.test.ignore_index = 3\n",
        "\n",
        "    if resume_file == '':\n",
        "        # Load an existing model\n",
        "        if model_type == 'deeplabv3plus':\n",
        "            cfg.load_from = 'checkpoints/deeplabv3plus_r50-d8_512x1024_80k_cityscapes_20200606_114049-f9fb496d.pth'\n",
        "        elif model_type == 'unet':\n",
        "            cfg.load_from = 'checkpoints/fcn_unet_s5-d16_4x4_512x1024_160k_cityscapes_20211210_145204-6860854e.pth'\n",
        "    else:\n",
        "        # or resume from a specific checkpoint\n",
        "        cfg.resume_from = os.path.join(work_dir, resume_file)\n",
        "\n",
        "    # Set up working dir to save files and logs.\n",
        "    cfg.work_dir = work_dir\n",
        "\n",
        "    # Choose number of iterations according to dataset size and batch size\n",
        "    cfg.runner.max_iters = 84000\n",
        "\n",
        "    # Choose log interval(every epoch)\n",
        "    cfg.log_config.interval = 210\n",
        "\n",
        "    # Evaluation interval(every 5 epochs)\n",
        "    cfg.evaluation.interval = 1050\n",
        "\n",
        "    # Save model with the highest validation mIoU\n",
        "    cfg.evaluation.save_best = 'mIoU'\n",
        "\n",
        "    # Also, choose checkpoint interval to save model each 10 epochs\n",
        "    cfg.checkpoint_config.interval = 2100\n",
        "\n",
        "    # Set seed to facitate reproducing the result\n",
        "    cfg.seed = 0\n",
        "    set_random_seed(0, deterministic=False)\n",
        "    cfg.gpu_ids = range(1)\n",
        "    cfg.device = get_device()\n",
        "\n",
        "    # Let's have a look at the final config used for training\n",
        "    print(f'Config:\\n{cfg.pretty_text}')\n",
        "\n",
        "    return cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWuH14LYF2gQ"
      },
      "source": [
        "## Train and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After setting the data, model checkpoints and configuration dictionary you can start training and evaluation."
      ],
      "metadata": {
        "id": "5humlPh4HMOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYKoSfdMF12B"
      },
      "outputs": [],
      "source": [
        "from mmseg.datasets import build_dataset\n",
        "from mmseg.models import build_segmentor\n",
        "from mmseg.apis import train_segmentor\n",
        "\n",
        "# Define configuration\n",
        "cfg = create_config(model_type, classes, data_root, img_dir, ann_dir, split_dir, work_dir, custom_img_scale, resume_file='iter_24990.pth')\n",
        "\n",
        "# Build the dataset\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "\n",
        "# Build the detector\n",
        "model = build_segmentor(cfg.model)\n",
        "# Add an attribute for visualization convenience\n",
        "model.CLASSES = datasets[0].CLASSES\n",
        "\n",
        "# Create work_dir\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "print(cfg.work_dir)\n",
        "train_segmentor(model, datasets, cfg, distributed=False, validate=True, \n",
        "                meta=dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "MQL4ZuwBVvtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "C_pr82uPnajP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some cases of our experiments, we cannot use only the provided API for inference as we may need an inference function that does not rescale the output image to the original size (we want to keep the size to the custom defined one)."
      ],
      "metadata": {
        "id": "TnufpGvNW4Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mmseg.apis.inference import LoadImage\n",
        "from mmseg.core.evaluation import get_palette\n",
        "from mmcv.parallel import collate, scatter\n",
        "from mmseg.datasets.pipelines import Compose\n",
        "import warnings\n",
        "\n",
        "\n",
        "def custom_inference_no_rescale(model, imgs):\n",
        "    \"\"\"Inference on custom image size\"\"\"\n",
        "\n",
        "    cfg = model.cfg\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # build the data pipeline\n",
        "    test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n",
        "    test_pipeline = Compose(test_pipeline)\n",
        "\n",
        "    # prepare data\n",
        "    data = []\n",
        "    imgs = imgs if isinstance(imgs, list) else [imgs]\n",
        "    for img in imgs:\n",
        "        img_data = dict(img=img)\n",
        "        img_data = test_pipeline(img_data)\n",
        "        data.append(img_data)\n",
        "    data = collate(data, samples_per_gpu=len(imgs))\n",
        "    if next(model.parameters()).is_cuda:\n",
        "        # scatter to specified GPU\n",
        "        data = scatter(data, [device])[0]\n",
        "    else:\n",
        "        data['img_metas'] = [i.data[0] for i in data['img_metas']]\n",
        "        \n",
        "    # forward the model\n",
        "    with torch.no_grad():\n",
        "        result = model(return_loss=False, rescale=False, **data)\n",
        "    return result\n",
        "\n",
        "def custom_result_pyplot(model, img, result, palette=None, fig_size=(15, 10),\n",
        "                         opacity=0.5, title='', block=True, out_file=None):\n",
        "    \"\"\"Plot prediction on custom image size\"\"\"\n",
        "\n",
        "    if hasattr(model, 'module'):\n",
        "        model = model.module\n",
        "    \n",
        "    show = False\n",
        "    img = mmcv.imread(img)\n",
        "    img_data = dict(img=img)\n",
        "    pipeline =  [dict(type='Resize', img_scale=[(1056, 792)], keep_ratio=False)]\n",
        "    plot_pipeline = Compose(pipeline)\n",
        "    img_data = plot_pipeline(img_data)\n",
        "    img = np.array(img_data['img'])\n",
        "\n",
        "    img = img.copy()\n",
        "    seg = result[0]\n",
        "\n",
        "    if palette is None:\n",
        "        # Get random state before set seed,\n",
        "        # and restore random state later.\n",
        "        # It will prevent loss of randomness, as the palette\n",
        "        # may be different in each iteration if not specified.\n",
        "        # See: https://github.com/open-mmlab/mmdetection/issues/5844\n",
        "        state = np.random.get_state()\n",
        "        np.random.seed(42)\n",
        "        # random palette\n",
        "        palette = np.random.randint(\n",
        "            0, 255, size=(len(classes), 3))\n",
        "        np.random.set_state(state)\n",
        "    else:\n",
        "        palette = palette\n",
        "        \n",
        "    palette = np.array(palette)\n",
        "    assert palette.shape[0] == len(classes)\n",
        "    assert palette.shape[1] == 3\n",
        "    assert len(palette.shape) == 2\n",
        "    assert 0 < opacity <= 1.0\n",
        "    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
        "    for label, color in enumerate(palette):\n",
        "        color_seg[seg == label, :] = color\n",
        "    # convert to BGR\n",
        "    color_seg = color_seg[..., ::-1]\n",
        "\n",
        "    img = img * (1 - opacity) + color_seg * opacity\n",
        "    img = img.astype(np.uint8)\n",
        "    # if out_file specified, do not show image in window\n",
        "    if out_file is not None:\n",
        "        show = False\n",
        "\n",
        "    if show:\n",
        "        mmcv.imshow(img, '', 0)\n",
        "    if out_file is not None:\n",
        "        mmcv.imwrite(img, out_file)\n",
        "\n",
        "    if not (show or out_file):\n",
        "        warnings.warn('show==False and out_file is not specified, only '\n",
        "                        'result image will be returned')\n",
        "            \n",
        "    plt.figure(figsize=fig_size)\n",
        "    plt.imshow(mmcv.bgr2rgb(img))\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show(block=block)\n",
        "    if out_file is not None:\n",
        "        mmcv.imwrite(img, out_file)\n",
        "\n",
        "\n",
        "def get_train_filenames(data_root, split_dir, train=True, val=True):\n",
        "    \"\"\"Gathers all train and/or val data used\"\"\"\n",
        "\n",
        "    splits_dir = os.path.join(data_root, split_dir)\n",
        "    data_val = []\n",
        "    data_train = []\n",
        "\n",
        "    if val:\n",
        "        with open(os.path.join(splits_dir, 'val.txt'), 'r') as f:\n",
        "            data_val = f.readlines()\n",
        "            data_val = [line[:-1] for line in data_val]\n",
        "    if train:\n",
        "        with open(os.path.join(splits_dir, 'train.txt'), 'r') as f:\n",
        "            data_train = f.readlines()\n",
        "            data_train = [line[:-1] for line in data_train]\n",
        "    \n",
        "\n",
        "    return sorted(data_train + data_val)\n",
        "\n",
        "\n",
        "def prepare_for_inference(custom_image_scale, checkpoint_file, palette, cfg=None):\n",
        "    \"\"\"Prepares the configuration file for inference\"\"\"\n",
        "    \n",
        "    if cfg is None:\n",
        "        raise ValueError('Config should be created first to pe prepared for inference')\n",
        "\n",
        "    # Create custom test pipeline with resizing\n",
        "    cfg.model.pretrained = None\n",
        "    cfg.model.train_cfg = None\n",
        "    cfg.load_from = None\n",
        "    cfg.resume_from = None\n",
        "    cfg.test_pipeline = [\n",
        "        dict(type='LoadImageFromFile'),\n",
        "        dict(type='Resize', img_scale=[custom_img_scale], keep_ratio=False),\n",
        "        dict(\n",
        "            type='MultiScaleFlipAug',\n",
        "            img_scale=custom_img_scale,\n",
        "            flip=False,\n",
        "            transforms=[\n",
        "                dict(type='Resize', keep_ratio=True),\n",
        "                dict(type='RandomFlip'),\n",
        "                dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "                dict(type='ImageToTensor', keys=['img']),\n",
        "                dict(type='Collect', keys=['img']),\n",
        "            ])\n",
        "    ]\n",
        "    cfg.data.test.pipeline = cfg.test_pipeline\n",
        "\n",
        "    # Build the model from a config file and load \n",
        "    model = mmseg.models.build_segmentor(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
        "    checkpoint = mmcv.runner.load_checkpoint(model, checkpoint_file, map_location='cpu')\n",
        "    model.CLASSES = checkpoint['meta']['CLASSES']\n",
        "    model.PALETTE = palette\n",
        "    model.cfg = cfg\n",
        "    model.to('cuda:0')\n",
        "    model.eval() \n",
        "\n",
        "    return model, cfg \n",
        "\n",
        "\n",
        "def get_best_checkpoint(work_dir):\n",
        "    best_checkpoint = None\n",
        "    files = sorted(os.listdir(work_dir))[::-1]\n",
        "    for file in files:\n",
        "        if 'best' in file:\n",
        "            best_checkpoint = file\n",
        "    if best_checkpoint is None:\n",
        "        raise ValueError(\"No best checkpoints were saved, please select 1 manually\")\n",
        "    else:\n",
        "        return os.path.join(work_dir, best_checkpoint)"
      ],
      "metadata": {
        "id": "Cmw0i67t89jo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference check on one image from validation set"
      ],
      "metadata": {
        "id": "SbfrAH3UW0OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an image to make inference on (here one image from validation set)\n",
        "img_name = 'DJI_0328_120.jpg'\n",
        "img = mmcv.imread(os.path.join(img_root, img_name))"
      ],
      "metadata": {
        "id": "MmonVvqjO5g5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekG__UfaH_OU"
      },
      "outputs": [],
      "source": [
        "# Create initial config\n",
        "cfg = create_config(model_type, classes, data_root, img_dir, ann_dir, split_dir, work_dir, custom_img_scale)\n",
        "\n",
        "# Choose a checkpoint for loading the trained weights\n",
        "checkpoint_file = get_best_checkpoint(work_dir)\n",
        "\n",
        "# Update config and model for inference\n",
        "model, cfg = prepare_for_inference(custom_img_scale, checkpoint_file, palette, cfg)\n",
        "\n",
        "# Do inference and plot\n",
        "result = inference_segmentor(model, img)\n",
        "show_result_pyplot(model, img, result, palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on multiple drone images"
      ],
      "metadata": {
        "id": "R5sAaUuI9UdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "AbYZ1AfOWD8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If prediction should be on full image size or custom image size (if False)\n",
        "fullsize = True\n",
        "\n",
        "# Choose subset to make prediction on\n",
        "train_img = False\n",
        "val_img = True\n",
        "\n",
        "# Define drone directory with images to make inference on:\n",
        "drone_dir = 'drone_dir_full'"
      ],
      "metadata": {
        "id": "V9Y75i2SV_A8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "LVYTm4QHaZf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: Results file should be created outside mmsegmentation."
      ],
      "metadata": {
        "id": "aJ43WReVcKDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU9-BHJFgICW"
      },
      "outputs": [],
      "source": [
        "# Define and create output folder for predictions\n",
        "result_dir = 'prediction_{}_{}_{}_{}{}'.format(model_type, img_dir, ann_dir, split_dir, '_full_size' if fullsize else '_custom_size')\n",
        "results_files_path = os.path.join(os.path.join('..', data_root), result_dir)\n",
        "mmcv.mkdir_or_exist(results_files_path)\n",
        "\n",
        "# Get all image names you want to make a prediction on\n",
        "files = get_train_filenames(data_root, split_dir, train_img, val_img)\n",
        "print(\"There are {} drone images\".format(len(files)))\n",
        "\n",
        "# Get directory with all drone images\n",
        "drone_files_path = os.path.join(os.path.join('..', data_root), drone_dir)\n",
        "print(\"Predicting on images from {}\".format(drone_files_path))\n",
        "\n",
        "# Create initial config\n",
        "cfg = create_config(model_type, classes, data_root, img_dir, ann_dir, split_dir, work_dir, custom_img_scale)\n",
        "\n",
        "# Choose a checkpoint for loading the trained weights\n",
        "checkpoint_file = get_best_checkpoint(work_dir)\n",
        "\n",
        "# Update config and model for inference\n",
        "model, cfg = prepare_for_inference(custom_img_scale, checkpoint_file, palette, cfg)\n",
        "\n",
        "for filename in files:\n",
        "    img_path = os.path.join(drone_files_path, filename + '.jpg')\n",
        "    img = mmcv.imread(img_path, backend='cv2')\n",
        "    # We need a sleep function otherwise Colab will crash\n",
        "    time.sleep(15)\n",
        "    if fullsize:\n",
        "        result = inference_segmentor(model, img)\n",
        "    else:\n",
        "        result = custom_inference_no_rescale(model, img)\n",
        "    result = np.array(result).transpose((1, 2, 0)).squeeze().astype(np.uint8)\n",
        "\n",
        "    # Convert prediction to 'P' mode\n",
        "    seg_img = Image.fromarray(result).convert('P')\n",
        "    seg_img.putpalette(np.array(palette, dtype=np.uint8))\n",
        "\n",
        "    # Save results\n",
        "    seg_img.save(os.path.join(results_files_path, filename.split('.')[0] + '.png'))\n",
        "    print(\"{} done!\".format(filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute mIoU between ground truths and predictions"
      ],
      "metadata": {
        "id": "_8vsqsgXo89F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mmseg.core.evaluation.metrics import mean_iou\n",
        "\n",
        "\n",
        "def threshold_img(img_hsv):\n",
        "    \"\"\"Creates categorical label based on HSV colospace thresholding\"\"\"\n",
        "\n",
        "    # define range of blue color in HSV\n",
        "    lower_blue = np.array([115, 50, 50])\n",
        "    upper_blue = np.array([125, 255, 255])\n",
        "    mask_blue = cv2.inRange(img_hsv, lower_blue, upper_blue)\n",
        "\n",
        "    # define range of green color in HSV\n",
        "    lower_green = np.array([55, 50, 50])\n",
        "    upper_green = np.array([65, 255, 255])\n",
        "    mask_green = cv2.inRange(img_hsv, lower_green, upper_green)\n",
        "\n",
        "    # lower mask (0-10)\n",
        "    lower_red = np.array([0, 50, 50])\n",
        "    upper_red = np.array([5, 255, 255])\n",
        "    mask0_red = cv2.inRange(img_hsv, lower_red, upper_red)\n",
        "\n",
        "    # upper mask (170-180)\n",
        "    lower_red = np.array([175, 50, 50])\n",
        "    upper_red = np.array([180, 255, 255])\n",
        "    mask1_red = cv2.inRange(img_hsv, lower_red, upper_red)\n",
        "\n",
        "    # join my masks\n",
        "    mask_red = mask0_red + mask1_red\n",
        "\n",
        "    # set my output img to zero everywhere except my mask\n",
        "    output_hsv = np.zeros((img_hsv.shape[0], img_hsv.shape[1]))\n",
        "    output_hsv[np.where(mask_red == 255)] = 1\n",
        "    output_hsv[np.where(mask_blue == 255)] = 2\n",
        "    output_hsv[np.where(mask_green == 255)] = 3\n",
        "\n",
        "    return output_hsv.astype(np.uint8)\n",
        "\n",
        "\n",
        "def compute_mIoU_between_gt_and_pred(ann_path, pred_path, data_root, splits_dir, scale, fullsize=False):\n",
        "    \"\"\"\n",
        "    Computes mIoU.\n",
        "    If fullsize is True, scale must be also specified.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all predicted (validation) filenames\n",
        "    predicted_files = sorted(get_train_filenames(data_root, splits_dir, train=False, val=True))\n",
        "\n",
        "    # Lists for gathering all images\n",
        "    ann_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    for filename in tqdm.tqdm(predicted_files):\n",
        "        filename += '.png'\n",
        "        if fullsize:\n",
        "            ann_label = np.array(Image.open(os.path.join(ann_path, filename)))\n",
        "        else:\n",
        "            # Scale image according to 'scale' parameter\n",
        "            label_ann = np.array(Image.open(os.path.join(ann_path, filename)).convert(\"RGB\"))[:, :, ::-1]\n",
        "            # time.sleep(15)\n",
        "            label_ann = cv2.resize(label_ann, scale, interpolation=cv2.INTER_AREA)\n",
        "            label_ann_hsv = cv2.cvtColor(label_ann, cv2.COLOR_BGR2HSV)\n",
        "            ann_label = threshold_img(label_ann_hsv)\n",
        "            \n",
        "        ann_labels.append(ann_label)\n",
        "        pred_label = np.array(Image.open(os.path.join(pred_path, filename)))\n",
        "        # time.sleep(5)\n",
        "        pred_labels.append(pred_label)\n",
        "    \n",
        "    # Compute mean IoU\n",
        "    score_dict = mean_iou(pred_labels,\n",
        "                        ann_labels,\n",
        "                        num_classes=4,\n",
        "                        ignore_index=-1,\n",
        "                        nan_to_num=None,\n",
        "                        label_map=dict(),\n",
        "                        reduce_zero_label=False)\n",
        "    \n",
        "    # Detele unwanted variables and return result\n",
        "    del ann_labels\n",
        "    del pred_labels\n",
        "\n",
        "    return score_dict"
      ],
      "metadata": {
        "id": "k2ZI9DiPBB8e"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mIoU between human drone labels and predicted drone labels (sat-sat, drone-sat, PatchMatch, LoFTR, etc.)"
      ],
      "metadata": {
        "id": "0VpDoFMLLaku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose human annotation directory\n",
        "drone_label_path = \"../data/120m/human_drone_ann_dir\"\n",
        "\n",
        "# Choose predictions directory\n",
        "pred_path = '../data/120m/prediction_deeplabv3plus_img_dir_ann_dir_splits_custom_size'\n",
        "\n",
        "score_dict = compute_mIoU_between_gt_and_pred(drone_label_path, pred_path, data_root,\n",
        "                                              split_dir, scale=custom_img_scale, fullsize='full_size' in pred_path)\n",
        "print(score_dict)"
      ],
      "metadata": {
        "id": "xzrGhfZNLVk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsfrrVh8VBJX"
      },
      "source": [
        "## Plot train loss and validation mIoU for a specific model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp3SE0X8IlPp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "log_path = os.path.join(work_dir, 'None.log.json')\n",
        "train_loss = []\n",
        "val_miou = []\n",
        "\n",
        "with open(log_path, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        data_dict = json.loads(line)\n",
        "        if 'hook_msgs' in data_dict.keys():\n",
        "            continue\n",
        "        else:\n",
        "            if data_dict['mode'] == 'train':\n",
        "                train_loss.append(data_dict['loss'])\n",
        "            elif data_dict['mode'] == 'val':\n",
        "                val_miou.append(data_dict['mIoU'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqGTkbf0nL5C"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy6nBT7Fphi1"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_miou)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xgp7_qyXSTmn"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('colab')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "407d2a53ddc3f8f7c4edd35e4d9b95b1c1ccdf5b3711df67dd21487022baf36e"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}